{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf5b3fe-341a-405d-aeac-af5406acc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考代码: https://www.kaggle.com/code/phanisrikanth/generate-synthetic-essays-with-mistral-7b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65b04ca-f674-438b-ae54-d620b809fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49451388-94be-4a2d-92aa-490f646d8e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mistral:\n",
    "    def __init__(self, model_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype = torch.bfloat16,\n",
    "            device_map = \"auto\",\n",
    "        )\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def generate(self, messages, max_new_tokens=1000):\n",
    "        \"\"\" 生成 LLM 回复\n",
    "        \n",
    "        Args:\n",
    "            messages (List[Dict]): chat template 模板格式\n",
    "                messages = [{\"role\": \"user\", \"content\": \"What do you think of the Chinese New Year ?\"},]\n",
    "                详见 https://huggingface.co/docs/transformers/main/en/chat_templating#how-do-i-use-chat-templates\n",
    "            max_new_tokens (int): 生成回复的最大长度\n",
    "        \"\"\"\n",
    "        token_chat = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        token_output = self.model.generate(\n",
    "            token_chat,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(token_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691506a5-ce84-4888-83f1-b0f707ed439b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e92cf6403a45a0bb69661c0f9edfbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhy/miniconda3/envs/llm/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "mistral = Mistral(\"./Mistral-7B-Instruct-v0.2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ae7df4-9220-44a4-b4fc-1719862bf56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>instructions</th>\n",
       "      <th>source_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Car-free cities</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "      <td># In German Suburb, Life Goes On Without Cars ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Does the electoral college work?</td>\n",
       "      <td>Write a letter to your state senator in which ...</td>\n",
       "      <td># What Is the Electoral College? by the Office...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_id                       prompt_name  \\\n",
       "0          0                   Car-free cities   \n",
       "1          1  Does the electoral college work?   \n",
       "\n",
       "                                        instructions  \\\n",
       "0  Write an explanatory essay to inform fellow ci...   \n",
       "1  Write a letter to your state senator in which ...   \n",
       "\n",
       "                                         source_text  \n",
       "0  # In German Suburb, Life Goes On Without Cars ...  \n",
       "1  # What Is the Electoral College? by the Office...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_df = pd.read_csv(\"train_prompts.csv\")\n",
    "prompt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97681d60-75bb-43f7-89f6-203b057337e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # 每个 prompt 生成的文章数\n",
    "    count = 1\n",
    "    # 使用 instructions 的概率\n",
    "    guidance_probability = .8\n",
    "    # 每篇文章的字数范围\n",
    "    words_range = (200, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d289993f-14e3-4a5d-b67f-7a1502c3a13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pages 1: 100%|██████████| 1/1 [00:34<00:00, 34.64s/it]\n",
      "Pages 2: 100%|██████████| 1/1 [00:30<00:00, 30.49s/it]\n"
     ]
    }
   ],
   "source": [
    "llm_text = []\n",
    "instruct = []\n",
    "prompt_text = \"\"\"\n",
    "You are a student working on the following assignment.\n",
    "\n",
    "Write an essay based on the following topics and backgrounds with a word count of about {0}.\n",
    "\n",
    "Topic: \"{1}\"\n",
    "\n",
    "Backgrounds: \"{2}\"\n",
    "\n",
    "\"\"\"\n",
    "for i in range(len(prompt_df)):\n",
    "    for page in tqdm.tqdm(range(CFG.count), f\"Pages {i + 1}\"):\n",
    "        prompt_name, instructions = prompt_df.loc[i, [\"prompt_name\", \"instructions\"]]\n",
    "        if np.random.random() > CFG.guidance_probability:\n",
    "            instructions = \"Feel free to use your imagination.\"\n",
    "        words = np.random.uniform(*CFG.words_range)\n",
    "        messages = [{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt_text.format(words, prompt_name, instructions)\n",
    "        }]\n",
    "        llm_generated = mistral.generate(messages=messages, max_new_tokens=2000)\n",
    "        llm_generated = llm_generated.split(\"[/INST]\")[1].rstrip(\"</s>\")\n",
    "        llm_text.append(llm_generated)\n",
    "        instruct.append(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "954d42e6-4624-4f72-84f7-94f60eeb6cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>text</th>\n",
       "      <th>instructions</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Car-free cities</td>\n",
       "      <td>Title: Embracing Car-free Cities: A Sustainab...</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Does the electoral college work?</td>\n",
       "      <td>Title: An In-depth Analysis of the Electoral ...</td>\n",
       "      <td>Feel free to use your imagination.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        prompt_name  \\\n",
       "0                   Car-free cities   \n",
       "1  Does the electoral college work?   \n",
       "\n",
       "                                                text  \\\n",
       "0   Title: Embracing Car-free Cities: A Sustainab...   \n",
       "1   Title: An In-depth Analysis of the Electoral ...   \n",
       "\n",
       "                                        instructions  generated  \n",
       "0  Write an explanatory essay to inform fellow ci...          1  \n",
       "1                 Feel free to use your imagination.          1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_generated_df = pd.DataFrame({\n",
    "    \"prompt_name\": prompt_df.prompt_name.unique().repeat(CFG.count),\n",
    "    \"text\": llm_text,\n",
    "    \"instructions\": instruct,\n",
    "    \"generated\": [1] * len(llm_text)\n",
    "})\n",
    "llm_generated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ccc4a60-436f-4752-9bcb-322d1946224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_generated_df.to_csv(\"llm_generated.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
